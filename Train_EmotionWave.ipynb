{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "N_uDiHL_28On"
      },
      "id": "N_uDiHL_28On"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load libraries and datasets"
      ],
      "metadata": {
        "id": "LH98vh7H0tVP"
      },
      "id": "LH98vh7H0tVP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151b2a45-dc41-45d4-9a7e-9db212d0ce00",
      "metadata": {
        "id": "151b2a45-dc41-45d4-9a7e-9db212d0ce00"
      },
      "outputs": [],
      "source": [
        "# @title Install and load libraries { display-mode: \"form\" }\n",
        "# @markdown The following libraries and programs will be installed:\n",
        "# @markdown - MIDItok (tokeniser)\n",
        "# @markdown - Fluidsynth (MIDI to wav)\n",
        "# @markdown - Muspy (Symbolic music libraries handle)\n",
        "# @markdown ---\n",
        "# @markdown All necessary libraries are included in this cell\n",
        "\n",
        "# Install MIDItok for tokenising MIDI files\n",
        "!pip install miditok\n",
        "# Library used to handle symbolic music datasets. Used in this case for\n",
        "# Emopia and Maestro datasets\n",
        "!pip install muspy\n",
        "# Fluidsynth for producing wav files from midi (using a soundfont)\n",
        "!pip install midi2audio\n",
        "!pip install fluidsynth\n",
        "# Install fluidsynth\n",
        "!apt install fluidsynth\n",
        "# Copy default sample of musical instruments to the current directory, for Colab.\n",
        "# Should be manually added in a jupyter file\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "\n",
        "# Using REMI scheme tokenisation\n",
        "from miditok import REMI, TokenizerConfig\n",
        "# Manage paths\n",
        "from pathlib import Path\n",
        "import os\n",
        "# Ipython.display to show audio files\n",
        "import IPython.display as ipd\n",
        "# Librosa for music and audio analysis\n",
        "import librosa\n",
        "# Symbolic music datasets library\n",
        "import muspy\n",
        "# To calculate information from the midi file\n",
        "import music21\n",
        "# To copy objects\n",
        "import copy\n",
        "# Pytorch\n",
        "import torch\n",
        "\n",
        "# Set datasets path\n",
        "datasets_folder = 'data'\n",
        "vgmidi_path  = datasets_folder + '/vgmidi'\n",
        "emopia_path  = datasets_folder + '/emopia'\n",
        "maestro_path = datasets_folder + '/maestro'\n",
        "\n",
        "# Create folder for datasets\n",
        "if not os.path.exists(datasets_folder):\n",
        "    os.makedirs(datasets_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261dcdd3-836d-43e3-a132-fbd647136247",
      "metadata": {
        "id": "261dcdd3-836d-43e3-a132-fbd647136247",
        "outputId": "1679d988-b9f8-4c59-a4bb-79eeea65afc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skip downloading as the `.muspy.success` file is found.\n",
            "Skip extracting as the `.muspy.success` file is found.\n",
            "Skip downloading as the `.muspy.success` file is found.\n",
            "Skip extracting as the `.muspy.success` file is found.\n",
            "VGMIDI unlabelled paths 3850\n",
            "VGMIDI labelled paths 204\n",
            "EMOPIA labelled paths 1071\n",
            "MAESTRO labelled paths 1276\n",
            "Combined labelled paths 1275\n",
            "Combined unlabelled paths 5126\n",
            "Combined paths 6401\n"
          ]
        }
      ],
      "source": [
        "# @title Get datasets { display-mode: \"form\" }\n",
        "# @markdown Datasets to download:\n",
        "# @markdown - VGMIDI (from github)\n",
        "# @markdown - EMOPIA (muspy)\n",
        "# @markdown - MAESTRO (muspy)\n",
        "# Not need to run if files already exist, in case of Colab notebook clone and\n",
        "# extract the files\n",
        "#!git clone https://github.com/lucasnfe/vgmidi.git {vgmidi_path}\n",
        "# Extract unlabelled midi from VGMIDI dataset\n",
        "#!unzip \"{vgmidi_path}/unlabelled/midi.zip\" -d \"{vgmidi_path}/\"\n",
        "\n",
        "# Use muspy library for emopia and maestro datasets\n",
        "emopia = muspy.EMOPIADataset(emopia_path, download_and_extract=True)\n",
        "maestro = muspy.datasets.MAESTRODatasetV3(maestro_path, download_and_extract=True)\n",
        "\n",
        "# Get paths of unlabelled data in VGMIDI dataset\n",
        "vgmidi_unlabelled_midi_paths = list(Path(\"./data/vgmidi/midi\").glob(\"*.mid\"))\n",
        "print(\"VGMIDI unlabelled paths\", len(vgmidi_unlabelled_midi_paths))\n",
        "\n",
        "# Get paths of labelled data in VGMIDI dataset\n",
        "vgmidi_labelled_midi_paths = list(Path(\"./data/vgmidi/labelled/midi\").glob(\"*.mid\"))\n",
        "print(\"VGMIDI labelled paths\", len(vgmidi_labelled_midi_paths))\n",
        "\n",
        "# Get paths of labelled data in EMOPIA dataset\n",
        "emopia_labelled_midi_paths = list(Path(\"./data/emopia/EMOPIA_2.2/midis\").glob(\"*.mid\"))\n",
        "print(\"EMOPIA labelled paths\", len(emopia_labelled_midi_paths))\n",
        "\n",
        "# Create list to store MAESTRO dataset paths\n",
        "maestro_unlabelled_midi_paths = list()\n",
        "# Get paths of unlabelled data in MAESTRO dataset\n",
        "for folder in os.listdir(\"./data/maestro/maestro-v3.0.0/\"):\n",
        "  maestro_unlabelled_midi_paths +=  list(Path(\"./data/maestro/maestro-v3.0.0/\" + folder).glob(\"*.mid*\"))\n",
        "print(\"MAESTRO labelled paths\", len(maestro_unlabelled_midi_paths))\n",
        "\n",
        "# Create list of combined laballed paths\n",
        "combined_labelled_dataset_paths = vgmidi_labelled_midi_paths + emopia_labelled_midi_paths\n",
        "print(\"Combined labelled paths\", len(combined_labelled_dataset_paths))\n",
        "\n",
        "# Create list of combined unlabelled paths\n",
        "combined_unlabelled_dataset_paths = vgmidi_unlabelled_midi_paths + maestro_unlabelled_midi_paths\n",
        "print(\"Combined unlabelled paths\", len(combined_unlabelled_dataset_paths))\n",
        "\n",
        "# Create list of combined labelled and unlabelled paths\n",
        "combined_dataset_paths = combined_unlabelled_dataset_paths + combined_labelled_dataset_paths\n",
        "print(\"Combined paths\", len(combined_dataset_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e289466b-1c65-4a8b-af3d-9921cfcdf628",
      "metadata": {
        "id": "e289466b-1c65-4a8b-af3d-9921cfcdf628"
      },
      "outputs": [],
      "source": [
        "# @title Clone Emotion Wave github repository { display-mode: \"form\" }\n",
        "!git clone https://github.com/JorgePdlR/EmotionWave.git\n",
        "\n",
        "# Set datasets path\n",
        "datasets_folder = 'data'\n",
        "output_folder = 'FolderData'\n",
        "\n",
        "truncated_folder = 'EmotionWave/vgmidi_unlabelled_truncated.zip'\n",
        "\n",
        "!unzip \"{truncated_folder}\" -d \"{output_folder}/\"\n",
        "dataset_path = output_folder + \"Vgmidi_unlabelled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60513dc8-73db-40e7-904b-1568fc95b7dd",
      "metadata": {
        "id": "60513dc8-73db-40e7-904b-1568fc95b7dd"
      },
      "outputs": [],
      "source": [
        "# @title Run to update the content from the github repository { display-mode: \"form\" }\n",
        "# @markdown This is a no-operation if no changes has been done\n",
        "%%capture\n",
        "%cd EmotionWave\n",
        "!git pull\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0084fad-18f3-4135-b685-2d8f49ce3ea2",
      "metadata": {
        "id": "a0084fad-18f3-4135-b685-2d8f49ce3ea2"
      },
      "outputs": [],
      "source": [
        "# @title Load tokeniser { display-mode: \"form\" }\n",
        "# Load tokeniser\n",
        "from miditok import TokSequence\n",
        "import miditok\n",
        "import importlib\n",
        "import EmotionWave.MIDIoperations\n",
        "from EmotionWave.MIDIoperations import REMItokenizer, MidiWav\n",
        "import pretty_midi\n",
        "from midi2audio import FluidSynth\n",
        "\n",
        "# Reload the module\n",
        "importlib.reload(EmotionWave.MIDIoperations)\n",
        "\n",
        "TOKENIZER_PARAMS = {\n",
        "    \"pitch_range\": (21, 109),\n",
        "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
        "    \"num_velocities\": 32,\n",
        "    \"use_chords\": True,\n",
        "    \"use_rests\": False,\n",
        "    \"use_tempos\": True,\n",
        "    \"use_time_signatures\": False,\n",
        "    \"use_programs\": False,\n",
        "    \"use_pitchdrum_tokens\": False,\n",
        "    \"num_tempos\": 32,  # number of tempo bins\n",
        "    \"tempo_range\": (40, 250),  # (min, max)\n",
        "}\n",
        "\n",
        "remi = REMItokenizer(TOKENIZER_PARAMS, max_bar_embedding=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b30c2fa7-8916-453e-9772-de9bd2a80273",
      "metadata": {
        "id": "b30c2fa7-8916-453e-9772-de9bd2a80273",
        "outputId": "8e6de097-5b52-4383-a25e-5f1522bf1999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8-bar sequences with 128 to remove 13595\n",
            "Size of 8-bar sequences 55468\n",
            "not 8-bar sequences to remove 4488\n",
            "Size of 8-bar sequences 50980\n",
            "no valence sequences to remove 0\n",
            "Size of 8-bar sequences with valence 50980\n"
          ]
        }
      ],
      "source": [
        "# @title Filter data from dataset { display-mode: \"form\" }\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# Load dataset\n",
        "#with open('data_8bar_ids_dict.pkl', 'rb') as pickle_file:\n",
        "with open('data_8bar_ids_dict_clean.pkl', 'rb') as pickle_file:\n",
        "    data_dict = pickle.load(pickle_file)\n",
        "\n",
        "# Remove all midi fragments that have sequences of tokens of more than 128\n",
        "# values\n",
        "val = 0\n",
        "to_remove_list = []\n",
        "# Remove all fragments with more than 128 tokens per bar\n",
        "for bar8_path, bar8_dict in data_dict.items():\n",
        "    for bar_path, bar_dict in bar8_dict.items():\n",
        "        if len(bar_dict['ids']) >= 128:\n",
        "            to_remove_list.append(bar8_path)\n",
        "            val += 1\n",
        "            # Break to avoid adding the same bar8_path multiple times\n",
        "            break\n",
        "\n",
        "print(\"8-bar sequences with 128 to remove\", val)\n",
        "\n",
        "# Remove the collected keys from the outer dictionary\n",
        "for path_to_remove in to_remove_list:\n",
        "    data_dict.pop(path_to_remove)\n",
        "\n",
        "print(\"Size of 8-bar sequences\", len(data_dict))\n",
        "\n",
        "# Remove all midi fragments that are not exactly 8 bars\n",
        "val = 0\n",
        "to_remove_list = []\n",
        "for bar8_path, bar8_dict in data_dict.items():\n",
        "    for bar_path, bar_dict in bar8_dict.items():\n",
        "        if len(bar8_dict) != 8:\n",
        "            to_remove_list.append(bar8_path)\n",
        "            val += 1\n",
        "            break\n",
        "\n",
        "print(\"not 8-bar sequences to remove\", val)\n",
        "\n",
        "# Remove the collected keys from the outer dictionary\n",
        "for path_to_remove in to_remove_list:\n",
        "    data_dict.pop(path_to_remove)\n",
        "\n",
        "print(\"Size of 8-bar sequences\", len(data_dict))\n",
        "\n",
        "# Remove midi fragments without valence\n",
        "val = 0\n",
        "to_remove_list = []\n",
        "for bar8_path, bar8_dict in data_dict.items():\n",
        "    for bar_path, bar_dict in bar8_dict.items():\n",
        "        if 'valence' not in bar_dict:\n",
        "            to_remove_list.append(bar8_path)\n",
        "            val += 1\n",
        "            break\n",
        "\n",
        "print(\"no valence sequences to remove\", val)\n",
        "\n",
        "# Remove the collected keys from the outer dictionary\n",
        "for path_to_remove in to_remove_list:\n",
        "    data_dict.pop(path_to_remove)\n",
        "\n",
        "print(\"Size of 8-bar sequences with valence\", len(data_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ceff046-aa1f-403e-b77f-97cccb50ae53",
      "metadata": {
        "id": "3ceff046-aa1f-403e-b77f-97cccb50ae53",
        "outputId": "2c855930-908c-4375-ef47-51fe3afa1a62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder input torch.Size([50980, 128, 8])\n",
            "Decoder input torch.Size([50980, 1024])\n",
            "Decoder target torch.Size([50980, 1024])\n",
            "Decoder bar position torch.Size([50980, 9])\n",
            "Padding mask torch.Size([50980, 8, 128])\n",
            "Valence per sequence torch.Size([50980, 1024])\n",
            "Valence statistics torch.Size([50980, 8])\n"
          ]
        }
      ],
      "source": [
        "# @title Format all the data for training EmotionWave { display-mode: \"form\" }\n",
        "import torch\n",
        "\n",
        "# Convert valence to int. 1 for negative, 2 for positive\n",
        "def map_valence_to_int(valence_val, max_val):\n",
        "    # 0 Value is used as padding\n",
        "    if valence_val > .5:\n",
        "        return int(2)\n",
        "    return int(1)\n",
        "\n",
        "vocab_dict = remi.get_vocab_dict()\n",
        "\n",
        "pad_ids = vocab_dict['PAD_None']\n",
        "bars_per_sample = 8\n",
        "sequence_per_bar_length = 128\n",
        "sequence_length = 1024\n",
        "valence_range = 2\n",
        "\n",
        "encoder_input_list = []\n",
        "decoder_input_list = []\n",
        "decoder_bar_position_list = []\n",
        "decoder_target_list = []\n",
        "padding_mask_list = []\n",
        "valence_cls_list = []\n",
        "valence_val_list = []\n",
        "\n",
        "for bar8_path, bar8_dict in data_dict.items():\n",
        "    # Initialise the tensor with the padding ids\n",
        "    # Max size of elements in bar, number of bars\n",
        "    encoder_input = torch.full((sequence_per_bar_length, bars_per_sample), pad_ids)\n",
        "    # Max size of elements in a sequence\n",
        "    decoder_input = torch.full((sequence_length,), pad_ids)\n",
        "    # Max size of number of bars, fixed to 8 + 1 including start-end position\n",
        "    decoder_bar_position = torch.zeros((bars_per_sample + 1,))\n",
        "    # Number of bars, max size of elements per bar sequence\n",
        "    padding_mask = torch.ones((bars_per_sample, sequence_per_bar_length), dtype=torch.bool)\n",
        "    # Shape of Max size of elements in a sequence.\n",
        "    valence_cls = torch.full((sequence_length,), pad_ids)\n",
        "    # For statistics of the valence\n",
        "    valence_val = torch.full((bars_per_sample,), pad_ids)\n",
        "\n",
        "    offset_decoder_input = 0\n",
        "    bar_seq_size = 0\n",
        "\n",
        "    # Go through all the individual bars\n",
        "    for z, (bar_path, bar_dict) in enumerate(bar8_dict.items()):\n",
        "        ids_t = torch.tensor(bar_dict['ids'])\n",
        "        ids_len = len(ids_t)\n",
        "        encoder_input[0:ids_len, z] = ids_t\n",
        "\n",
        "        padding_mask[z, 0:ids_len] = False\n",
        "\n",
        "        decoder_input[offset_decoder_input: offset_decoder_input + ids_len] = ids_t\n",
        "        valence_cls[offset_decoder_input: offset_decoder_input + ids_len] = map_valence_to_int(bar_dict['valence'], valence_range)\n",
        "        valence_val[z] = map_valence_to_int(bar_dict['valence'], valence_range)\n",
        "        offset_decoder_input += ids_len\n",
        "        # Adding one because first starting position is 0, we have 8 ranges\n",
        "        # that correspond to 9 positions\n",
        "        decoder_bar_position[z + 1] = offset_decoder_input\n",
        "\n",
        "        bar_seq_size += ids_len\n",
        "\n",
        "\n",
        "    # Store in a list the encoder input tensor\n",
        "    encoder_input_list.append(encoder_input)\n",
        "\n",
        "    # Store in a list the decoder input tensor\n",
        "    decoder_input_list.append(decoder_input)\n",
        "\n",
        "    # Store the decoder target tensor in a list\n",
        "    decoder_target = torch.cat((decoder_input[1:], torch.tensor([pad_ids])))\n",
        "    decoder_target_list.append(decoder_target)\n",
        "\n",
        "    # Store in a list the decoder bar start end position\n",
        "    decoder_bar_position_list.append(decoder_bar_position)\n",
        "\n",
        "    # Store the padding values per bar in a list\n",
        "    padding_mask_list.append(padding_mask)\n",
        "\n",
        "    # Store in a list the valence values per sequence\n",
        "    valence_cls_list.append(valence_cls)\n",
        "\n",
        "    # Store in a list the valence values without expansions for statistics\n",
        "    valence_val_list.append(valence_val)\n",
        "\n",
        "# Stack the tensors along a new dimension\n",
        "encoder_input_tensor = torch.stack(encoder_input_list)\n",
        "decoder_input_tensor = torch.stack(decoder_input_list)\n",
        "decoder_target_tensor = torch.stack(decoder_target_list)\n",
        "decoder_bar_position_tensor = torch.stack(decoder_bar_position_list)\n",
        "padding_mask_tensor = torch.stack(padding_mask_list)\n",
        "valence_cls_tensor = torch.stack(valence_cls_list)\n",
        "valence_val_tensor = torch.stack(valence_val_list)\n",
        "\n",
        "print(\"Encoder input\", encoder_input_tensor.shape)\n",
        "print(\"Decoder input\", decoder_input_tensor.shape)\n",
        "print(\"Decoder target\", decoder_target_tensor.shape)\n",
        "print(\"Decoder bar position\", decoder_bar_position_tensor.shape)\n",
        "print(\"Padding mask\", padding_mask_tensor.shape)\n",
        "# In valence per sequence the same valence value is replicated over the complete\n",
        "# bar. Each bar might have a different valence. Valence is mapped to integers\n",
        "# in the provided range\n",
        "print(\"Valence per sequence\", valence_cls_tensor.shape)\n",
        "print(\"Valence statistics\", valence_val_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11905be-b5f7-4320-9dc7-2a51594d7c05",
      "metadata": {
        "id": "f11905be-b5f7-4320-9dc7-2a51594d7c05",
        "outputId": "419f026d-5db0-47dd-d1c3-92d7ac7a32e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train dataset shapes:\n",
            "  encoder_input: torch.Size([40784, 128, 8])\n",
            "  decoder_input: torch.Size([40784, 1024])\n",
            "  decoder_target: torch.Size([40784, 1024])\n",
            "  decoder_bar_position: torch.Size([40784, 9])\n",
            "  padding_mask: torch.Size([40784, 8, 128])\n",
            "  valence_cls: torch.Size([40784, 1024])\n",
            "\n",
            "Validation dataset shapes:\n",
            "  encoder_input: torch.Size([5098, 128, 8])\n",
            "  decoder_input: torch.Size([5098, 1024])\n",
            "  decoder_target: torch.Size([5098, 1024])\n",
            "  decoder_bar_position: torch.Size([5098, 9])\n",
            "  padding_mask: torch.Size([5098, 8, 128])\n",
            "  valence_cls: torch.Size([5098, 1024])\n",
            "\n",
            "Test dataset shapes:\n",
            "  encoder_input: torch.Size([5098, 128, 8])\n",
            "  decoder_input: torch.Size([5098, 1024])\n",
            "  decoder_target: torch.Size([5098, 1024])\n",
            "  decoder_bar_position: torch.Size([5098, 9])\n",
            "  padding_mask: torch.Size([5098, 8, 128])\n",
            "  valence_cls: torch.Size([5098, 1024])\n"
          ]
        }
      ],
      "source": [
        "# @title Create train, test and validation sets { display-mode: \"form\" }\n",
        "\n",
        "def print_dataset_shapes(dataset_dict, name):\n",
        "    print(f\"\\n{name} dataset shapes:\")\n",
        "    for key, tensor in dataset_dict.items():\n",
        "        print(f\"  {key}: {tensor.shape}\")\n",
        "\n",
        "def shuffle_dataset(dataset):\n",
        "    # Get the number of samples\n",
        "    num_samples = next(iter(dataset.values())).shape[0]\n",
        "\n",
        "    # Generate a random permutation of indices\n",
        "    indices = torch.randperm(num_samples)\n",
        "\n",
        "    # Create a new dictionary with shuffled data\n",
        "    shuffled_dataset = {key: tensor[indices] for key, tensor in dataset.items()}\n",
        "\n",
        "    return shuffled_dataset\n",
        "\n",
        "num_files = encoder_input_tensor.shape[0]\n",
        "\n",
        "# Create a dictionary of the dataset\n",
        "emotionwave_dataset = {\n",
        "    'encoder_input': encoder_input_tensor,\n",
        "    'decoder_input': decoder_input_tensor,\n",
        "    'decoder_target': decoder_target_tensor,\n",
        "    'decoder_bar_position': decoder_bar_position_tensor,\n",
        "    'padding_mask': padding_mask_tensor,\n",
        "    'valence_cls': valence_cls_tensor,\n",
        "}\n",
        "\n",
        "emotionwave_dataset = shuffle_dataset(emotionwave_dataset)\n",
        "\n",
        "# Calculate from which file the validation set starts\n",
        "valid_start = round(0.8 * num_files)\n",
        "# Calculate from which file the test set starts\n",
        "test_start = round(0.9 * num_files)\n",
        "\n",
        "# Create train, validation, and test dictionaries\n",
        "train_data = {\n",
        "    'encoder_input': encoder_input_tensor[:valid_start],\n",
        "    'decoder_input': decoder_input_tensor[:valid_start],\n",
        "    'decoder_target': decoder_target_tensor[:valid_start],\n",
        "    'decoder_bar_position': decoder_bar_position_tensor[:valid_start],\n",
        "    'padding_mask': padding_mask_tensor[:valid_start],\n",
        "    'valence_cls': valence_cls_tensor[:valid_start],\n",
        "}\n",
        "\n",
        "valid_data = {\n",
        "    'encoder_input': encoder_input_tensor[valid_start:test_start],\n",
        "    'decoder_input': decoder_input_tensor[valid_start:test_start],\n",
        "    'decoder_target': decoder_target_tensor[valid_start:test_start],\n",
        "    'decoder_bar_position': decoder_bar_position_tensor[valid_start:test_start],\n",
        "    'padding_mask': padding_mask_tensor[valid_start:test_start],\n",
        "    'valence_cls': valence_cls_tensor[valid_start:test_start],\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "    'encoder_input': encoder_input_tensor[test_start:],\n",
        "    'decoder_input': decoder_input_tensor[test_start:],\n",
        "    'decoder_target': decoder_target_tensor[test_start:],\n",
        "    'decoder_bar_position': decoder_bar_position_tensor[test_start:],\n",
        "    'padding_mask': padding_mask_tensor[test_start:],\n",
        "    'valence_cls': valence_cls_tensor[test_start:],\n",
        "}\n",
        "\n",
        "# Print information about train, validation and test\n",
        "print_dataset_shapes(train_data, \"Train\")\n",
        "print_dataset_shapes(valid_data, \"Validation\")\n",
        "print_dataset_shapes(test_data, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa18b73d-b153-4b53-bd56-b5f25eaa5183",
      "metadata": {
        "id": "aa18b73d-b153-4b53-bd56-b5f25eaa5183",
        "outputId": "f4e83e01-7252-4bdf-b42b-e5f286215714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Dataset Information:\n",
            "Full dataset shapes:\n",
            "  encoder_input: torch.Size([40784, 128, 8])\n",
            "  decoder_input: torch.Size([40784, 1024])\n",
            "  decoder_target: torch.Size([40784, 1024])\n",
            "  decoder_bar_position: torch.Size([40784, 9])\n",
            "  padding_mask: torch.Size([40784, 8, 128])\n",
            "  valence_cls: torch.Size([40784, 1024])\n",
            "\n",
            "Shapes of tensors in the first batch:\n",
            "  encoder_input: torch.Size([4, 128, 8])\n",
            "  decoder_input: torch.Size([4, 1024])\n",
            "  decoder_target: torch.Size([4, 1024])\n",
            "  decoder_bar_position: torch.Size([4, 9])\n",
            "  padding_mask: torch.Size([4, 8, 128])\n",
            "  valence_cls: torch.Size([4, 1024])\n",
            "\n",
            "Number of batches: 10196\n",
            "Batch size: 4\n",
            "Total number of samples: 40784\n",
            "\n",
            "Validation Dataset Information:\n",
            "Full dataset shapes:\n",
            "  encoder_input: torch.Size([5098, 128, 8])\n",
            "  decoder_input: torch.Size([5098, 1024])\n",
            "  decoder_target: torch.Size([5098, 1024])\n",
            "  decoder_bar_position: torch.Size([5098, 9])\n",
            "  padding_mask: torch.Size([5098, 8, 128])\n",
            "  valence_cls: torch.Size([5098, 1024])\n",
            "\n",
            "Shapes of tensors in the first batch:\n",
            "  encoder_input: torch.Size([4, 128, 8])\n",
            "  decoder_input: torch.Size([4, 1024])\n",
            "  decoder_target: torch.Size([4, 1024])\n",
            "  decoder_bar_position: torch.Size([4, 9])\n",
            "  padding_mask: torch.Size([4, 8, 128])\n",
            "  valence_cls: torch.Size([4, 1024])\n",
            "\n",
            "Number of batches: 1275\n",
            "Batch size: 4\n",
            "Total number of samples: 5098\n",
            "\n",
            "Test Dataset Information:\n",
            "Full dataset shapes:\n",
            "  encoder_input: torch.Size([5098, 128, 8])\n",
            "  decoder_input: torch.Size([5098, 1024])\n",
            "  decoder_target: torch.Size([5098, 1024])\n",
            "  decoder_bar_position: torch.Size([5098, 9])\n",
            "  padding_mask: torch.Size([5098, 8, 128])\n",
            "  valence_cls: torch.Size([5098, 1024])\n",
            "\n",
            "Shapes of tensors in the first batch:\n",
            "  encoder_input: torch.Size([4, 128, 8])\n",
            "  decoder_input: torch.Size([4, 1024])\n",
            "  decoder_target: torch.Size([4, 1024])\n",
            "  decoder_bar_position: torch.Size([4, 9])\n",
            "  padding_mask: torch.Size([4, 8, 128])\n",
            "  valence_cls: torch.Size([4, 1024])\n",
            "\n",
            "Number of batches: 1275\n",
            "Batch size: 4\n",
            "Total number of samples: 5098\n"
          ]
        }
      ],
      "source": [
        "# @title Create loaders { display-mode: \"form\" }\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "\n",
        "class DictDataset(Dataset):\n",
        "    def __init__(self, tensor_dict):\n",
        "        self.tensor_dict = tensor_dict\n",
        "        self.keys = list(tensor_dict.keys())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {key: tensor[index] for key, tensor in self.tensor_dict.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(next(iter(self.tensor_dict.values())))\n",
        "\n",
        "def create_dataloader(data_dict, batch_size, shuffle=False):\n",
        "    # Convert all tensors to torch tensors if they aren't already\n",
        "    tensor_dict = {k: torch.tensor(v) if not isinstance(v, torch.Tensor) else v for k, v in data_dict.items()}\n",
        "\n",
        "    # Create a DictDataset\n",
        "    dataset = DictDataset(tensor_dict)\n",
        "\n",
        "    # Create and return a DataLoader\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def print_dataset_info(data_dict, loader, name):\n",
        "    print(f\"\\n{name} Dataset Information:\")\n",
        "\n",
        "    # Print full dataset shapes\n",
        "    print(\"Full dataset shapes:\")\n",
        "    for key, tensor in data_dict.items():\n",
        "        print(f\"  {key}: {tensor.shape}\")\n",
        "\n",
        "    # Print shapes of the first batch\n",
        "    for batch in loader:\n",
        "        print(\"\\nShapes of tensors in the first batch:\")\n",
        "        for key, tensor in batch.items():\n",
        "            print(f\"  {key}: {tensor.shape}\")\n",
        "        break  # We only want to see the first batch\n",
        "\n",
        "    print(f\"\\nNumber of batches: {len(loader)}\")\n",
        "    print(f\"Batch size: {loader.batch_size}\")\n",
        "    total_samples = len(loader.dataset)\n",
        "    print(f\"Total number of samples: {total_samples}\")\n",
        "\n",
        "# Size of each batch\n",
        "batch_size = 4\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = create_dataloader(train_data, batch_size, shuffle=True)\n",
        "valid_loader = create_dataloader(valid_data, batch_size, shuffle=False)\n",
        "test_loader = create_dataloader(test_data, batch_size, shuffle=False)\n",
        "\n",
        "# Print information for each dataset\n",
        "print_dataset_info(train_data, train_loader, \"Training\")\n",
        "print_dataset_info(valid_data, valid_loader, \"Validation\")\n",
        "print_dataset_info(test_data, test_loader, \"Test\")\n",
        "\n",
        "\n",
        "# Store datasets\n",
        "#with open('./pickle_data/train_data.pkl', 'wb') as pickle_file:\n",
        "#    pickle.dump(train_data, pickle_file)\n",
        "\n",
        "#with open('./pickle_data/valid_data.pkl', 'wb') as pickle_file:\n",
        "#    pickle.dump(valid_data, pickle_file)\n",
        "\n",
        "#with open('./pickle_data/test_data.pkl', 'wb') as pickle_file:\n",
        "#    pickle.dump(test_data, pickle_file)\n",
        "\n",
        "#with open('./pickle_data/train_loader.pkl', 'wb') as pickle_file:\n",
        "#    pickle.dump(train_loader, pickle_file)\n",
        "\n",
        "#with open('./pickle_data/valid_loader.pkl', 'wb') as pickle_file:\n",
        "#    pickle.dump(valid_loader, pickle_file)\n",
        "\n",
        "#with open('./pickle_data/test_loader.pkl', 'wb') as pickle_file:\n",
        "#    pickle.dump(test_loader, pickle_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EmotionWave training\n",
        "Time to train EmotionWave, log files will be created with the training information"
      ],
      "metadata": {
        "id": "idhnfKf51vx9"
      },
      "id": "idhnfKf51vx9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63cad9e-ae1d-4f58-b54d-94726d372e15",
      "metadata": {
        "id": "d63cad9e-ae1d-4f58-b54d-94726d372e15"
      },
      "outputs": [],
      "source": [
        "# @title Training functions { display-mode: \"form\" }\n",
        "import logging\n",
        "import os\n",
        "from logging.handlers import RotatingFileHandler\n",
        "import numpy as np\n",
        "\n",
        "# Get information about the tensor, debugging purposes\n",
        "def print_tensor_info(ptensor):\n",
        "    # Type of the tensor object\n",
        "    print(type(ptensor))\n",
        "\n",
        "    # Data type of the elements in the tensor\n",
        "    print(ptensor.dtype)\n",
        "\n",
        "    # Shape of the tensor\n",
        "    print(ptensor.shape)\n",
        "\n",
        "    # Device of the tensor (CPU or GPU)\n",
        "    print(ptensor.device)\n",
        "\n",
        "# Evaluate the model provided the loader and the number of rounds for the evaluation\n",
        "def evaluate(model, data_loader, n_rounds=8, valence_cnd=True):\n",
        "    model.eval()\n",
        "    loss_recons = []\n",
        "    kl_loss_recons = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_rounds):\n",
        "\n",
        "            # Go through all the data\n",
        "            for batch_id, batch_inputs in enumerate(data_loader):\n",
        "                model.zero_grad()\n",
        "\n",
        "                # Put data in the GPU and accommodate the columns\n",
        "                encoder_input = batch_inputs['encoder_input'].permute(1,0,2).to(device, dtype=torch.long)\n",
        "                decoder_input = batch_inputs['decoder_input'].permute(1,0).to(device, dtype=torch.long)\n",
        "                decoder_target = batch_inputs['decoder_target'].permute(1,0).to(device, dtype=torch.long)\n",
        "                decoder_bar_position = batch_inputs['decoder_bar_position'].to(device, dtype=torch.int)\n",
        "                padding_mask = batch_inputs['padding_mask'].to(device, dtype=torch.bool)\n",
        "\n",
        "                if valence_cnd:\n",
        "                    valence_cls = batch_inputs['valence_cls'].permute(1,0).to(device, dtype=torch.long)\n",
        "                else:\n",
        "                    valence_cls = None\n",
        "\n",
        "                # Get the model outputs: mu (mean), logvar (log variance), and decoder logits\n",
        "                mu, logvar, decoder_logits = model(encoder_input, decoder_input,\n",
        "                                                decoder_bar_position, valence_cls,\n",
        "                                                padding_mask=padding_mask)\n",
        "\n",
        "                loss = model.compute_loss(mu, logvar, 0.0, 0.0, decoder_logits, decoder_target)\n",
        "\n",
        "                loss_recons.append(loss['recons_loss'].item())\n",
        "                kl_loss_recons.append(loss['kldiv_raw'].item())\n",
        "\n",
        "    return loss_recons, kl_loss_recons\n",
        "\n",
        "# Compute the exponential moving average loss\n",
        "def compute_loss_ema(ema, batch_loss, decay=0.95):\n",
        "    # If ema is zero (initial condition), return the batch loss\n",
        "    if ema == 0.:\n",
        "        return batch_loss\n",
        "    else:\n",
        "        # Compute the exponentially moving average (EMA) of the loss\n",
        "        # EMA is updated as a weighted average of the current batch loss and the\n",
        "        # previous EMA\n",
        "        return batch_loss * (1 - decay) + ema * decay\n",
        "\n",
        "# Compute beta cyclical scheduler\n",
        "def beta_cyclical_schedulder(step, kl_cycle_steps=5000, no_kl_steps=10000,\n",
        "                             kl_max_beta=1.0):\n",
        "    # Calculate the current step within the KL cycle\n",
        "    step_in_cycle = (step - 1) % kl_cycle_steps\n",
        "\n",
        "    # Determine the progress within the current cycle as a fraction\n",
        "    cycle_progress = step_in_cycle / kl_cycle_steps\n",
        "\n",
        "    # If the current step is within the initial no-KL steps, return 0\n",
        "    if step < no_kl_steps:\n",
        "        return 0.\n",
        "\n",
        "    # If the cycle progress is less than 0.5, calculate beta proportionally\n",
        "    if cycle_progress < 0.5:\n",
        "        return kl_max_beta * cycle_progress * 2.\n",
        "    else:\n",
        "        # If the cycle progress is greater than or equal to 0.5, return the maximum beta\n",
        "        return kl_max_beta\n",
        "\n",
        "# Train function, time to train the model!! Using log file to avoid issues with console output\n",
        "def train(model, train_loader, valid_loader, optimizer, num_epochs, saved_model,\n",
        "          log_file, evaluate_every_n_epochs=1, scheduler=None, constant_kl=False,\n",
        "          lr_warmup_steps=1000, kl_max_beta=1.0, free_bit_lambda=0.25, max_lr=1.0e-4,\n",
        "          verbose=False, valence_cnd=True):\n",
        "\n",
        "\n",
        "    # Setting training logger\n",
        "    handler = RotatingFileHandler(log_file, maxBytes=10**6, backupCount=5)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "\n",
        "    # Remove any existing handlers\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    # Remove any existing handlers to avoid printing to console\n",
        "    logger.handlers = []\n",
        "    # Adding file handler\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "    # Log the start of training\n",
        "    logger.info('Training started')\n",
        "\n",
        "    # Indicate that we are training the model, gradients will be calculated\n",
        "    model.train()\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    # Keep track of the best accuracy on the validation data\n",
        "    best_valid_acc = 0.0\n",
        "\n",
        "    # Keep count of trained steps\n",
        "    trained_steps = 0\n",
        "\n",
        "    # Initialize the exponentially moving averages (EMAs) for different loss\n",
        "    # components. Initially set to 0, indicating no prior information\n",
        "    # EMA for reconstruction loss\n",
        "    recons_loss_ema = 0\n",
        "    # EMA for KL divergence loss (weighted by beta)\n",
        "    kl_loss_ema = 0\n",
        "    # EMA for raw KL divergence loss (unweighted)\n",
        "    kl_raw_ema = 0\n",
        "\n",
        "    # Train for the provided number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Go through all the data\n",
        "        for batch_id, batch_inputs in enumerate(train_loader):\n",
        "            # Put data in the GPU and accommodate the columns\n",
        "            encoder_input = batch_inputs['encoder_input'].permute(1,0,2).to(device, dtype=torch.long)\n",
        "            decoder_input = batch_inputs['decoder_input'].permute(1,0).to(device, dtype=torch.long)\n",
        "            decoder_target = batch_inputs['decoder_target'].permute(1,0).to(device, dtype=torch.long)\n",
        "            decoder_bar_position = batch_inputs['decoder_bar_position'].to(device, dtype=torch.int)\n",
        "            padding_mask = batch_inputs['padding_mask'].to(device, dtype=torch.bool)\n",
        "\n",
        "            if valence_cnd:\n",
        "                valence_cls = batch_inputs['valence_cls'].permute(1,0).to(device, dtype=torch.long)\n",
        "            else:\n",
        "                valence_cls = None\n",
        "\n",
        "            # Forward + backward + optimize\n",
        "            # Get the model outputs: mu (mean), logvar (log variance), and decoder logits\n",
        "            mu, logvar, decoder_logits = model(encoder_input, decoder_input,\n",
        "                                               decoder_bar_position, valence_cls,\n",
        "                                               padding_mask=padding_mask, verbose=verbose)\n",
        "\n",
        "            # Increment the number of training steps\n",
        "            trained_steps += 1\n",
        "\n",
        "            # Determine the KL beta value\n",
        "            if constant_kl:\n",
        "                # If constant KL is used, set kl_beta to the maximum value\n",
        "                kl_beta = kl_max_beta\n",
        "            else:\n",
        "                # Otherwise, use a cyclical scheduler to get the current kl_beta\n",
        "                kl_beta = beta_cyclical_schedulder(trained_steps)\n",
        "\n",
        "            # Compute the loss using the model's loss function\n",
        "            loss = model.compute_loss(mu, logvar, kl_beta, free_bit_lambda,\n",
        "                                      decoder_logits, decoder_target)\n",
        "\n",
        "            # Adjust the learning rate based on the number of training steps\n",
        "            if trained_steps < lr_warmup_steps:\n",
        "                # During the warmup phase, linearly increase the learning rate\n",
        "                curr_lr = max_lr * trained_steps / lr_warmup_steps\n",
        "                # Set the current learning rate for the optimizer\n",
        "                optimizer.param_groups[0]['lr'] = curr_lr\n",
        "            else:\n",
        "                # After the warmup phase, use the scheduler to adjust the learning rate\n",
        "                scheduler.step()\n",
        "\n",
        "            # Set gradients of model parameters to zero before propagating\n",
        "            optimizer.zero_grad()\n",
        "            # Propagate the loss\n",
        "            loss['total_loss'].backward()\n",
        "\n",
        "            # Clip the gradients to prevent exploding gradients\n",
        "            # This limits the norm of the gradients to a maximum value of 0.5\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "\n",
        "            # Update model parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            recons_loss_ema = compute_loss_ema(recons_loss_ema, loss['recons_loss'].item())\n",
        "            kl_loss_ema = compute_loss_ema(kl_loss_ema, loss['kldiv_loss'].item())\n",
        "            kl_raw_ema = compute_loss_ema(kl_raw_ema, loss['kldiv_raw'].item())\n",
        "\n",
        "            logger.info(f'kl_beta {kl_beta}, batch_id {batch_id}, trained_steps {trained_steps}')\n",
        "            logger.info(f'loss_recons {recons_loss_ema}, kl_loss_ema {kl_loss_ema}, kl_raw_ema {kl_raw_ema}')\n",
        "\n",
        "\n",
        "        logger.info(f'[{epoch+1}]')\n",
        "\n",
        "        # Evaluate the network on the validation data\n",
        "        if((epoch+1) % evaluate_every_n_epochs == 0):\n",
        "            valloss = evaluate(model, valid_loader, valence_cnd=valence_cnd)\n",
        "            logger.info(f'Validation loss: train_steps {trained_steps}, loss_recons {np.mean(valloss[0])}, kl_raw_ema {np.mean(valloss[1])}')\n",
        "            model.train()\n",
        "\n",
        "        # Save the model and optimizer to a file\n",
        "        model_name = saved_model + '_' + str(epoch+1) + '_' + str(recons_loss_ema) + str(kl_raw_ema) + '.pt'\n",
        "        optimizer_name = saved_model + '_' + str(epoch+1) + '_' + str(recons_loss_ema) + str(kl_raw_ema) + '_optim.pt'\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "        torch.save(optimizer.state_dict(), optimizer_name)\n",
        "\n",
        "    testloss = evaluate(model, test_loader, valence_cnd=valence_cnd)\n",
        "    logger.info(f'Test loss: loss_recons {np.mean(testloss[0])}, kl_raw_ema {np.mean(testloss[1])}')\n",
        "\n",
        "    logger.info('Training completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb55e45-56c7-48c5-aa77-15224fc79370",
      "metadata": {
        "id": "9bb55e45-56c7-48c5-aa77-15224fc79370",
        "outputId": "6b08f2ca-229e-4b9d-9e31-823ee8f7635c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Vocabulary size: 267\n",
            "EmotionWave(\n",
            "  (input_embedding): EmbeddingWithProjection(\n",
            "    (embedding_lookup): Embedding(267, 512)\n",
            "  )\n",
            "  (positional_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): VAETransformerEncoder(\n",
            "    (transformer_encoder_layer): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-7): 8 x TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (vae_mu): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (vae_logvar): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (decoder): VAETransformerDecoder(\n",
            "    (condition_embedding_proj): Linear(in_features=192, out_features=512, bias=False)\n",
            "    (decoder_layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (valence_embedding): EmbeddingWithProjection(\n",
            "    (embedding_lookup): Embedding(3, 64)\n",
            "  )\n",
            "  (decoder_out_projection): Linear(in_features=512, out_features=267, bias=True)\n",
            ")\n",
            "Total trainable parameters: 41484491\n"
          ]
        }
      ],
      "source": [
        "# @title Train the model and print general information about it. Check log file for progress { display-mode: \"form\" }\n",
        "import EmotionWave.Model.EmotionWave as emw\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# Empty the CUDA cache to free up GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model_dir = 'Models'\n",
        "log_dir = 'Logs'\n",
        "log_file = 'log'\n",
        "\n",
        "vocab_size = len(remi.get_vocab_dict())\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "valence_cls=True\n",
        "\n",
        "# Model\n",
        "emw_model = emw.EmotionWave(8,          # Encoder number of layers\n",
        "                            512,        # Encoder dimensions\n",
        "                            8,          # Encoder number of heads\n",
        "                            2048,       # Encoder dimensions feedforward\n",
        "                            128,        # Dimensions latent VAE\n",
        "                            512,        # Dimensions embedding\n",
        "                            vocab_size, # Number of embeddings\n",
        "                            4,          # Decoder number of layers\n",
        "                            512,        # Decoder dimensions\n",
        "                            8,          # Decoder number of heads\n",
        "                            2048,       # Valence number CLS\n",
        "                            valence_dim_embeddings=64, # Valence embedding dimensions\n",
        "                            valence_num_cls=3, # Valence vocabulary\n",
        "                            valence_cls=valence_cls\n",
        "                            )\n",
        "\n",
        "print(emw_model)\n",
        "# Get total number of parameters\n",
        "total_params = count_parameters(emw_model)\n",
        "print(f\"Total trainable parameters: {total_params}\")\n",
        "emw_model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(emw_model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08)\n",
        "# Scheduler\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 70000, eta_min=5.0e-6)\n",
        "\n",
        "# Training epochs\n",
        "num_epochs = 20\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "#pretrained_params_path = './Models_20_0.72904897893097350.199615425882635.pt'\n",
        "#emw_model.load_state_dict(torch.load(pretrained_params_path))\n",
        "\n",
        "#optimizer_path = './Models_20_0.72904897893097350.199615425882635_optim.pt'\n",
        "#optimizer.load_state_dict(torch.load(optimizer_path))\n",
        "\n",
        "# How often the network will be evaluated during training\n",
        "evaluate_every_n_epochs = 5\n",
        "\n",
        "train(emw_model, train_loader, valid_loader, optimizer, num_epochs, model_dir, log_file,\n",
        "      evaluate_every_n_epochs=evaluate_every_n_epochs, scheduler=scheduler, constant_kl=False,\n",
        "      lr_warmup_steps=200, kl_max_beta=1.0, free_bit_lambda=0.25, max_lr=1.0e-4, verbose=False, valence_cnd=valence_cls)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}